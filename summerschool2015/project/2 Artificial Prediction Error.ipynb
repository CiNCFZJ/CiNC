{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Artificial Predicion Error"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Learning from reward and punishment driven by prediction error signal \u2013 artificial non-neural implementation."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We saw now that WTA dynamics can be interpreted as action selection and tuning the weights can influence which actions are selected in certain given states. Now how can we let the neural circuit tune these weights from positive or negative experience made in a task, so that it learns to select those actions that lead to rewards and to avoid those actions that lead to punishment?\n",
      "\n",
      "For that we need to define a learning or update rule that changes the weights in reasonable way. We also need to define what does it mean to obtain reward or to get a punishment.\n",
      "\n",
      "To demonstrate how this may work in a simple setting, we install an environment called \u201cgrid world\u201d. It has discrete  state an agent controlled by the neural network can be in, and actions that can be executed in each state that moves the agent from one grid square field to another. So it is a very simple version of a labyrinth task. \n",
      "\n",
      "If we define some of the grids of the grid world to contain a reward, we can provide the network with a reward signal. The same can be done for punishment.\n",
      "\n",
      "In frame of TD learning that we had introduced in the lecture, we need to keep track of so called outcome expectations, or Q-values, that has to be update each time the network experiences something it has not been able to properly predict. These updates are driven by a so called prediction error signal that is generated each time the network encounters an unpredicted reward or punishment."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.) 1 State - 2 Actions"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We work now on a very simple grid world instance consisting only of three states \u2013 the starting state and two states that the agent can reach from the start by executing one of two possible actions, move left or move right. This is equivalent to forced binary choice task, where a decision between two alternatives has to be made. One of the states will provide reward, the other state offers nothing. In spirit of a particular TD learning instantiation called Q-learning, create an array containing a Q(s,a) value for each state-action, or in other words, for each synaptic connection from a state pool to an action pool. Implement the equation to compute prediction error given the reward signal and the Q values. Is there a way of making the prediction error equation for this particular binary choice task even more simple than its full form? Implement update equation that changes the Q values based on the computed prediction error. How can the Q values be used now to make WTA circuit do the proper decisions about the actions to select given the state signals? Implement the necessary operation that utilizes the Q values in the suitable way."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import nest\n",
      "import nest.raster_plot as rplt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import environment as env"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "As in the exercise before, we set up a simple environment and create a WTA circuit to choose actions within this environment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "env.set_environment(0)\n",
      "\n",
      "\n",
      "NUM_ITERATIONS = 50\n",
      "LEARNING_RATE = 0.05\n",
      "NUM_STATE_NEURONS = 20\n",
      "NUM_WTA_NEURONS = 50\n",
      "WEIGHT_SCALING = 100 / NUM_STATE_NEURONS\n",
      "\n",
      "nest.ResetKernel()\n",
      "nest.set_verbosity(\"M_DEBUG\")\n",
      "\n",
      "rank = nest.Rank()\n",
      "size = nest.NumProcesses() \n",
      "seed = np.random.randint(0, 1000000)\n",
      "num_threads = 4\n",
      "nest.SetKernelStatus({\"local_num_threads\": num_threads})\n",
      "nest.SetKernelStatus({\"rng_seeds\": range(seed+num_threads * size + 1, seed + 2 * (num_threads * size) + 1),\n",
      "        \t\t      \"grng_seed\": seed+size+num_threads,\n",
      "                      \"resolution\": 0.1})\n",
      "\n",
      "# Create states\n",
      "world_dim = env.get_world_dimensions()\n",
      "states = []\n",
      "for i in range(world_dim['x']):\n",
      "    states.append(nest.Create('iaf_psc_alpha', NUM_STATE_NEURONS))\n",
      "all_states = np.ravel(states).tolist()\n",
      "\n",
      "# Create actions\n",
      "num_actions = env.get_num_possible_actions()\n",
      "actions = []\n",
      "for i in range(num_actions):\n",
      "    actions.append(nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS))\n",
      "all_actions = np.ravel(actions).tolist()\n",
      "\n",
      "# Create WTA circuit\n",
      "wta_ex_weights = 10.5\n",
      "wta_inh_weights = -2.6\n",
      "wta_ex_inh_weights = 2.8\n",
      "wta_noise_weights = 2.1\n",
      "\n",
      "wta_inh_neurons = nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS)\n",
      "\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], actions[i], 'all_to_all', {'weight': wta_ex_weights})\n",
      "    nest.Connect(actions[i], wta_inh_neurons, 'all_to_all', {'weight': wta_ex_inh_weights}) \n",
      "\n",
      "nest.Connect(wta_inh_neurons, all_actions, 'all_to_all', {'weight': wta_inh_weights})\n",
      "\n",
      "wta_noise = nest.Create('poisson_generator', 10, {'rate': 3000.})\n",
      "nest.Connect(wta_noise, all_actions, 'all_to_all', {'weight': wta_noise_weights})\n",
      "nest.Connect(wta_noise, wta_inh_neurons, 'all_to_all', {'weight': wta_noise_weights * 0.9})\n",
      "\n",
      "# Create noise\n",
      "noise = nest.Create('poisson_generator', 1, {'rate': 65000.})\n",
      "nest.Connect(noise, all_states, 'all_to_all', {'weight': 1.})\n",
      "\n",
      "# Create stimulus\n",
      "stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "position = env.get_agent_pos()['x']\n",
      "nest.Connect(stimulus, states[position], 'all_to_all', {'weight': 1.})\n",
      "\n",
      "# Create spike detector\n",
      "sd_wta = nest.Create('spike_detector')\n",
      "nest.Connect(all_actions, sd_wta)\n",
      "sd_actions = nest.Create('spike_detector', num_actions)\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], [sd_actions[i]])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "raw",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This time we estimate the valence of each action in 'values' and connect the state population to the action populations accordingly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Value expectations for Q(s,a)\n",
      "values = np.zeros(num_actions)\n",
      "    \n",
      "# Connect states to actions\n",
      "nest.Connect(states[1], actions[0], 'all_to_all', {'weight': values[0]})\n",
      "nest.Connect(states[1], actions[1], 'all_to_all', {'weight': values[1]})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "After each selected action, this function is called which will update the valence of the state."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_values(chosen_action, outcome):\n",
      "    # prediction error\n",
      "    prediction_error = outcome - values[chosen_action]\n",
      "\n",
      "    # update values\n",
      "    values[chosen_action] += prediction_error * LEARNING_RATE  \n",
      "\n",
      "    # save values for plotting\n",
      "    values_hist.append(values.copy())\n",
      "    return prediction_error\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "And finally we run the simulation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Main loop\n",
      "values_hist = [values.copy()]\n",
      "actions_executed = 0\n",
      "last_action_time = 0\n",
      "in_end_position = False\n",
      "\n",
      "while actions_executed < NUM_ITERATIONS:\n",
      "    if not in_end_position:\n",
      "        nest.SetStatus(wta_noise, {'rate': 3000.})\n",
      "        nest.Simulate(200)\n",
      "        max_rate = -1\n",
      "        chosen_action = -1\n",
      "        for i in range(len(sd_actions)):\n",
      "            rate = len([e for e in nest.GetStatus([sd_actions[i]], keys='events')[0]['times'] if e > last_action_time]) # calc the \"firerate\" of each actor population\n",
      "            if rate > max_rate:\n",
      "                max_rate = rate # the population with the hightes rate wins\n",
      "                chosen_action = i\n",
      "        nest.SetStatus(stimulus, {'rate': 5000.})\n",
      "\n",
      "        possible_actions = env.get_possible_actions() \n",
      "\n",
      "        _, outcome, in_end_position = env.move(possible_actions[chosen_action])\n",
      "\n",
      "        prediction_error = update_values(chosen_action, outcome)\n",
      "        print \"iteration:\", actions_executed, \"action:\", chosen_action, \"reward:\", outcome, \"updated values\", values, \"predicion error:\", prediction_error\n",
      "\n",
      "\n",
      "        for i in range(num_actions):\n",
      "            nest.SetStatus(nest.GetConnections(states[1], actions[i]), {'weight': values[i] * WEIGHT_SCALING})\n",
      "\n",
      "        nest.SetStatus(wta_noise, {'rate': 0.})\n",
      "        nest.Simulate(50.)\n",
      "\n",
      "        last_action_time += 250\n",
      "        actions_executed += 1\n",
      "    else:\n",
      "        _, in_end_position = env.init_new_trial()\n",
      "\n",
      "       \n",
      "plt.xlabel(\"# action\")\n",
      "plt.ylabel(\"valence\")\n",
      "plt.title(\"valence of each action\")\n",
      "plt.plot(values_hist)\n",
      "rplt.from_device(sd_wta, title=\"WTA circuit\")\n",
      "rplt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration: 0 action: 0 reward: 1.0 updated values [ 0.05  0.  ] predicion error: 1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 action: 0 reward: 1.0 updated values [ 0.0975  0.    ] predicion error: 0.95\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 action: 0 reward: 1.0 updated values [ 0.142625  0.      ] predicion error: 0.9025\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 action: 0 reward: 1.0 updated values [ 0.18549375  0.        ] predicion error: 0.857375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 action: 0 reward: 1.0 updated values [ 0.22621906  0.        ] predicion error: 0.81450625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 action: 0 reward: 1.0 updated values [ 0.26490811  0.        ] predicion error: 0.7737809375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 action: 0 reward: 1.0 updated values [ 0.3016627  0.       ] predicion error: 0.735091890625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 action: 0 reward: 1.0 updated values [ 0.33657957  0.        ] predicion error: 0.698337296094\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 action: 0 reward: 1.0 updated values [ 0.36975059  0.        ] predicion error: 0.663420431289\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 action: 1 reward: -1.0 updated values [ 0.36975059 -0.05      ] predicion error: -1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 action: 0 reward: 1.0 updated values [ 0.40126306 -0.05      ] predicion error: 0.630249409725\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11 action: 0 reward: 1.0 updated values [ 0.43119991 -0.05      ] predicion error: 0.598736939238\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12 action: 0 reward: 1.0 updated values [ 0.45963991 -0.05      ] predicion error: 0.568800092276\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13 action: 0 reward: 1.0 updated values [ 0.48665792 -0.05      ] predicion error: 0.540360087663\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14 action: 0 reward: 1.0 updated values [ 0.51232502 -0.05      ] predicion error: 0.51334208328\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15 action: 0 reward: 1.0 updated values [ 0.53670877 -0.05      ] predicion error: 0.487674979116\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 16 action: 0 reward: 1.0 updated values [ 0.55987333 -0.05      ] predicion error: 0.46329123016\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 17 action: 0 reward: 1.0 updated values [ 0.58187966 -0.05      ] predicion error: 0.440126668652\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18 action: 0 reward: 1.0 updated values [ 0.60278568 -0.05      ] predicion error: 0.418120335219\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 19 action: 0 reward: 1.0 updated values [ 0.6226464 -0.05     ] predicion error: 0.397214318458\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20 action: 0 reward: 1.0 updated values [ 0.64151408 -0.05      ] predicion error: 0.377353602535\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 21 action: 0 reward: 1.0 updated values [ 0.65943837 -0.05      ] predicion error: 0.358485922409\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 22 action: 0 reward: 1.0 updated values [ 0.67646646 -0.05      ] predicion error: 0.340561626288\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 23 action: 0 reward: 1.0 updated values [ 0.69264313 -0.05      ] predicion error: 0.323533544974\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24 action: 0 reward: 1.0 updated values [ 0.70801098 -0.05      ] predicion error: 0.307356867725\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25 action: 0 reward: 1.0 updated values [ 0.72261043 -0.05      ] predicion error: 0.291989024339\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 26 action: 0 reward: 1.0 updated values [ 0.73647991 -0.05      ] predicion error: 0.277389573122\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 27 action: 0 reward: 1.0 updated values [ 0.74965591 -0.05      ] predicion error: 0.263520094466\n",
        "iteration:"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise:\n",
      "\n",
      "How can the learning progress be visualized? What do you expected the Q values and prediction error to evolve like in course of learning? Provide plots that give insight about the learning progress. Experiment around with providing different reward magnitudes or having a punishing state instead of a neutral one. Describe you observations. Are there any issues with learning from punishment? If yes, is there any workaround you can think of? If there are no issues \u2013 fine. ))\n",
      "\n",
      "Change the iterations and learning rate. What is the pro and contra of high learning rates?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.) Multi State - 2 Actions"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "!!! PLEASE RESTART KERNEL HERE !!!\n",
      "\n",
      "In this example show how to handle multiple states.\n",
      "\n",
      "We extend the world to have more states than only three. The intermediate states has no consequence, the two final states \u2013 most left and most right \u2013 can be chosen to provide a reward and punishment or reward and nothing. \n",
      "\n",
      "Exercise: What is the crucial different between the task where the consequence \u2013 reward or punishment \u2013 is provided immediately after executing an action and the task where the consequence is delayed? Do the same kind of analysis on the learning progress providing the plots."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nest\n",
      "import nest.raster_plot as rplt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "import environment as env\n",
      "from mpl_toolkits.mplot3d import Axes3D"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "env.set_environment(1)\n",
      "\n",
      "\n",
      "NUM_ITERATIONS = 30\n",
      "LEARNING_RATE = 0.5\n",
      "NUM_STATE_NEURONS = 20\n",
      "NUM_WTA_NEURONS = 50\n",
      "WEIGHT_SCALING = 100 / NUM_STATE_NEURONS\n",
      "\n",
      "nest.ResetKernel()\n",
      "nest.set_verbosity(\"M_DEBUG\")\n",
      "\n",
      "rank = nest.Rank()\n",
      "size = nest.NumProcesses() \n",
      "seed = np.random.randint(0, 1000000)\n",
      "num_threads = 4\n",
      "nest.SetKernelStatus({\"local_num_threads\": num_threads})\n",
      "nest.SetKernelStatus({\"rng_seeds\": range(seed+num_threads * size + 1, seed + 2 * (num_threads * size) + 1),\n",
      "        \t\t      \"grng_seed\": seed+size+num_threads,\n",
      "                      \"resolution\": 0.1})\n",
      "\n",
      "\n",
      "# Create states\n",
      "world_dim = env.get_world_dimensions()\n",
      "states = []\n",
      "for i in range(world_dim['x']):\n",
      "    states.append(nest.Create('iaf_psc_alpha', NUM_STATE_NEURONS))\n",
      "all_states = np.ravel(states).tolist()\n",
      "\n",
      "# Create actions\n",
      "num_actions = env.get_num_possible_actions()\n",
      "actions = []\n",
      "for i in range(num_actions):\n",
      "    actions.append(nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS))\n",
      "all_actions = np.ravel(actions).tolist()\n",
      "\n",
      "# Create WTA circuit\n",
      "wta_ex_weights = 10.5\n",
      "wta_inh_weights = -2.6\n",
      "wta_ex_inh_weights = 2.8\n",
      "wta_noise_weights = 2.1\n",
      "\n",
      "wta_inh_neurons = nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS)\n",
      "\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], actions[i], 'all_to_all', {'weight': wta_ex_weights})\n",
      "    nest.Connect(actions[i], wta_inh_neurons, 'all_to_all', {'weight': wta_ex_inh_weights}) \n",
      "\n",
      "nest.Connect(wta_inh_neurons, all_actions, 'all_to_all', {'weight': wta_inh_weights})\n",
      "\n",
      "wta_noise = nest.Create('poisson_generator', 10, {'rate': 3000.})\n",
      "nest.Connect(wta_noise, all_actions, 'all_to_all', {'weight': wta_noise_weights})\n",
      "nest.Connect(wta_noise, wta_inh_neurons, 'all_to_all', {'weight': wta_noise_weights * 0.9})\n",
      "\n",
      "# Create noise\n",
      "noise = nest.Create('poisson_generator', 1, {'rate': 65000.})\n",
      "nest.Connect(noise, all_states, 'all_to_all', {'weight': 1.})\n",
      "\n",
      "# Create stimulus\n",
      "stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "nest.Connect(stimulus, all_states, 'all_to_all', {'weight': 0.})\n",
      "\n",
      "# Create spike detector\n",
      "sd_wta = nest.Create('spike_detector')\n",
      "nest.Connect(all_actions, sd_wta)\n",
      "sd_actions = nest.Create('spike_detector', num_actions)\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], [sd_actions[i]])\n",
      "sd_states = nest.Create('spike_detector')\n",
      "nest.Connect(all_states, sd_states)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Create values for each action in each state of the environment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Value expectations for Q(s,a)\n",
      "values = np.array([np.zeros(num_actions) for i in range(world_dim['x'])])\n",
      "        \n",
      "# Connect states to actions with initial weight 0.0\n",
      "nest.Connect(all_states, all_actions, 'all_to_all', {'weight': 0.0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gamma = 0.8\n",
      "\n",
      "def update_values(position, chosen_action, new_position, outcome):\n",
      "    # prediction error\n",
      "    best_new_action = values[new_position].argmax()\n",
      "    prediction_error = outcome + gamma * values[new_position][best_new_action] - values[position][chosen_action]\n",
      "\n",
      "    # update values\n",
      "    values[position][chosen_action] += prediction_error * LEARNING_RATE \n",
      "\n",
      "    # save values for plotting\n",
      "    values_hist.append(np.ravel(values.copy()))\n",
      "    \n",
      "    return prediction_error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_values(fig, ax, position):\n",
      "    plt.cla()\n",
      "    \n",
      "    values_plot = [[]]\n",
      "\n",
      "    \n",
      "    for i in range(len(states)):\n",
      "        values_plot[0].append(max(values[i]))\n",
      "    \n",
      "    xlabels = np.arange(0, len(states))\n",
      "    ylabels = np.arange(0, len(states[0]))\n",
      "\n",
      "    # Set the major ticks at the centers and minor tick at the edges\n",
      "    xlocs = np.arange(len(xlabels))\n",
      "    ylocs = np.arange(len(ylabels))\n",
      "    ax.xaxis.set_ticks(xlocs + 0.5, minor=True)\n",
      "    ax.xaxis.set(ticks=xlocs, ticklabels=xlabels)\n",
      "    ax.yaxis.set_ticks(ylocs + 0.5, minor=True)\n",
      "    ax.yaxis.set(ticks=ylocs, ticklabels=ylabels)\n",
      "    \n",
      "    # Turn on the grid for the minor ticks\n",
      "    ax.grid(True, which='minor', linestyle='-', linewidth=2)   \n",
      "    plt.imshow(values_plot, interpolation='None', vmin=-1, vmax=1)\n",
      "    \n",
      "    for txt in ax.texts:\n",
      "        txt.set_visible(False)\n",
      "        \n",
      "    ax.annotate(\".\", ((position['x'] + 0.5)/len(states), 0.5), size=160, textcoords='axes fraction')\n",
      "    plt.draw()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Main loop\n",
      "values_hist = [np.ravel(values.copy())]\n",
      "actions_executed = 0\n",
      "last_action_time = 0\n",
      "in_end_position = False\n",
      "\n",
      "# interactive plotting\n",
      "fig, ax = plt.subplots()\n",
      "plt.ion()\n",
      "\n",
      "\n",
      "\n",
      "while actions_executed < NUM_ITERATIONS:\n",
      "    position = env.get_agent_pos().copy()    \n",
      "    \n",
      "    # plotting\n",
      "    plot_values(fig, ax, position)\n",
      "    \n",
      "    if not in_end_position:        \n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']]), {'weight': 1.})\n",
      "        \n",
      "        nest.SetStatus(wta_noise, {'rate': 3000.})\n",
      "        nest.Simulate(200)\n",
      "        max_rate = -1\n",
      "        chosen_action = -1\n",
      "        for i in range(len(sd_actions)):\n",
      "            rate = len([e for e in nest.GetStatus([sd_actions[i]], keys='events')[0]['times'] if e > last_action_time]) # calc the \"firerate\" of each actor population\n",
      "            if rate > max_rate:\n",
      "                max_rate = rate # the population with the hightes rate wins\n",
      "                chosen_action = i\n",
      "        nest.SetStatus(stimulus, {'rate': 5000.})\n",
      "\n",
      "        possible_actions = env.get_possible_actions() \n",
      "\n",
      "        new_position, outcome, in_end_position = env.move(possible_actions[chosen_action])\n",
      "\n",
      "        prediction_error = update_values(position['x'], chosen_action, new_position['x'], outcome)\n",
      "        \n",
      "        print \"iteration:\", actions_executed, \"action:\", chosen_action\n",
      "        print \"new position:\", new_position['x'], \"reward:\", outcome, \"updated_values:\", values[position['x']], \"prediction error:\", prediction_error\n",
      "\n",
      "        for i in range(num_actions):\n",
      "            nest.SetStatus(nest.GetConnections(states[position['x']], actions[i]), {'weight': values[position['x']][i] * WEIGHT_SCALING})\n",
      "            \n",
      "        # stimulate new state\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']]), {'weight': 0.})\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[new_position['x']]), {'weight': 1.})\n",
      "\n",
      "        nest.SetStatus(wta_noise, {'rate': 0.})\n",
      "        nest.Simulate(50.)\n",
      "\n",
      "        last_action_time += 250\n",
      "        actions_executed += 1\n",
      "    else:\n",
      "        _, in_end_position = env.init_new_trial()\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']]), {'weight': 0.})\n",
      "        \n",
      "\n",
      "    \n",
      "   \n",
      "#rplt.from_device(sd_wta, title=\"WTA circuit\")\n",
      "#rplt.from_device(sd_states, title=\"states\")\n",
      "#rplt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.) Grid World --- Multiple States - 4 Actions"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "!!! PLEASE RESTART KERNEL HERE !!!\n",
      "\n",
      "Exercise : We extend the world to have a 2D grid of NxN size. We have more possible actions now for each state \u2013 4 instead of 2 (down, up, left, right). Do the same kind of analysis on the learning progress providing the plots."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nest\n",
      "import nest.raster_plot as rplt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import environment as env\n",
      "from mpl_toolkits.mplot3d import Axes3D"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "env.set_environment(7)\n",
      "\n",
      "NUM_ITERATIONS = 2000\n",
      "LEARNING_RATE = 0.5\n",
      "NUM_STATE_NEURONS = 20\n",
      "NUM_WTA_NEURONS = 50\n",
      "WEIGHT_SCALING = 100 / NUM_STATE_NEURONS\n",
      "\n",
      "nest.ResetKernel()\n",
      "nest.set_verbosity(\"M_DEBUG\")\n",
      "\n",
      "rank = nest.Rank()\n",
      "size = nest.NumProcesses() \n",
      "seed = np.random.randint(0, 1000000)\n",
      "num_threads = 4\n",
      "nest.SetKernelStatus({\"local_num_threads\": num_threads})\n",
      "nest.SetKernelStatus({\"rng_seeds\": range(seed+num_threads * size + 1, seed + 2 * (num_threads * size) + 1),\n",
      "        \t\t      \"grng_seed\": seed+size+num_threads,\n",
      "                      \"resolution\": 0.1})\n",
      "\n",
      "# Create states\n",
      "world_dim = env.get_world_dimensions()\n",
      "states = []\n",
      "for i in range(world_dim['x']):\n",
      "    states.append([])\n",
      "    for j in range(world_dim['y']):\n",
      "        states[i].append(nest.Create('iaf_psc_alpha', NUM_STATE_NEURONS))\n",
      "all_states = np.ravel(states).tolist()\n",
      "\n",
      "# Create actions\n",
      "num_actions = env.get_num_possible_actions()\n",
      "actions = []\n",
      "for i in range(num_actions):\n",
      "    actions.append(nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS))\n",
      "all_actions = np.ravel(actions).tolist()\n",
      "\n",
      "# Create WTA circuit\n",
      "wta_ex_weights = 10.5\n",
      "wta_inh_weights = -2.6\n",
      "wta_ex_inh_weights = 2.8\n",
      "wta_noise_weights = 2.1\n",
      "\n",
      "wta_inh_neurons = nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS)\n",
      "\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], actions[i], 'all_to_all', {'weight': wta_ex_weights})\n",
      "    nest.Connect(actions[i], wta_inh_neurons, 'all_to_all', {'weight': wta_ex_inh_weights}) \n",
      "\n",
      "nest.Connect(wta_inh_neurons, all_actions, 'all_to_all', {'weight': wta_inh_weights})\n",
      "\n",
      "wta_noise = nest.Create('poisson_generator', 10, {'rate': 3000.})\n",
      "nest.Connect(wta_noise, all_actions, 'all_to_all', {'weight': wta_noise_weights})\n",
      "nest.Connect(wta_noise, wta_inh_neurons, 'all_to_all', {'weight': wta_noise_weights * 0.9})\n",
      "\n",
      "# Create noise\n",
      "noise = nest.Create('poisson_generator', 1, {'rate': 65000.})\n",
      "nest.Connect(noise, all_states, 'all_to_all', {'weight': 1.})\n",
      "\n",
      "# Create stimulus\n",
      "stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "nest.Connect(stimulus, all_states, 'all_to_all', {'weight': 0.})\n",
      "\n",
      "# Create spike detector\n",
      "sd_wta = nest.Create('spike_detector')\n",
      "nest.Connect(all_actions, sd_wta)\n",
      "nest.Connect(wta_inh_neurons, sd_wta)\n",
      "sd_actions = nest.Create('spike_detector', num_actions)\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], [sd_actions[i]])\n",
      "sd_states = nest.Create('spike_detector')\n",
      "nest.Connect(all_states, sd_states)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "values = []\n",
      "# Value expectations for Q(s,a)\n",
      "for i in range(world_dim['x']):\n",
      "    values.append([])\n",
      "    for j in range(world_dim['y']):\n",
      "        values[i].append(np.zeros(num_actions))\n",
      "        \n",
      "# Connect states to actions with initial weight 0.0\n",
      "nest.Connect(all_states, all_actions, 'all_to_all', {'weight': 0.0})\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gamma = 0.8\n",
      "\n",
      "def update_values(position, chosen_action, new_position, outcome):\n",
      "    # prediction error\n",
      "    best_new_action = values[new_position['x']][new_position['y']].argmax()\n",
      "    prediction_error = outcome + gamma * values[new_position['x']][new_position['y']][best_new_action] - values[position['x']][position['y']][chosen_action]\n",
      "    \n",
      "    # update values\n",
      "    values[position['x']][position['y']][chosen_action] += prediction_error * LEARNING_RATE \n",
      "    return prediction_error\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_values(fig, ax, position):\n",
      "    plt.cla()\n",
      "    \n",
      "    values_plot = []      \n",
      "    \n",
      "    for i in range(world_dim['y']):\n",
      "        values_plot.append([])\n",
      "        for j in range(world_dim['x']):\n",
      "            values_plot[i].append(np.max(values[j][i]))\n",
      "    \n",
      "    values_plot = np.array(values_plot)\n",
      "    \n",
      "    plt.imshow(values_plot, interpolation='none', vmax= 1, vmin = -1)\n",
      "    \n",
      "    xlabels = np.arange(0, len(states))\n",
      "    ylabels = np.arange(0, len(states[0]))\n",
      "\n",
      "    # Set the major ticks at the centers and minor tick at the edges\n",
      "    xlocs = np.arange(len(xlabels))\n",
      "    ylocs = np.arange(len(ylabels))\n",
      "    ax.xaxis.set_ticks(xlocs + 0.5, minor=True)\n",
      "    ax.xaxis.set(ticks=xlocs, ticklabels=xlabels)\n",
      "    ax.yaxis.set_ticks(ylocs + 0.5, minor=True)\n",
      "    ax.yaxis.set(ticks=ylocs, ticklabels=ylabels)\n",
      "    \n",
      "    # Turn on the grid for the minor ticks\n",
      "    ax.grid(True, which='minor', linestyle='-', linewidth=2)   \n",
      "    \n",
      "    for txt in ax.texts:\n",
      "        txt.set_visible(False)\n",
      "        \n",
      "    ax.annotate(\".\", ((position['x'] + 0.5)/len(states), (1-(position['y'] + 0.5)/len(states[0]))), size=160, textcoords='axes fraction', color='white')\n",
      "    plt.draw()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Main loop\n",
      "actions_executed = 0\n",
      "last_action_time = 0\n",
      "in_end_position = False\n",
      "\n",
      "# interactive plotting\n",
      "fig, ax = plt.subplots()\n",
      "plt.ion()\n",
      "    \n",
      "while actions_executed < NUM_ITERATIONS:\n",
      "    position = env.get_agent_pos().copy()\n",
      "    # plotting\n",
      "    plot_values(fig, ax, position)\n",
      "    \n",
      "    if not in_end_position:        \n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 1.})\n",
      "        \n",
      "        nest.SetStatus(wta_noise, {'rate': 3000.})\n",
      "        nest.Simulate(200)\n",
      "        max_rate = -1\n",
      "        chosen_action = -1\n",
      "        for i in range(len(sd_actions)):\n",
      "            rate = len([e for e in nest.GetStatus([sd_actions[i]], keys='events')[0]['times'] if e > last_action_time]) # calc the \"firerate\" of each actor population\n",
      "            if rate > max_rate:\n",
      "                max_rate = rate # the population with the hightes rate wins\n",
      "                chosen_action = i\n",
      "        nest.SetStatus(stimulus, {'rate': 5000.})\n",
      "\n",
      "        possible_actions = env.get_possible_actions() \n",
      "\n",
      "        new_position, outcome, in_end_position = env.move(possible_actions[chosen_action])\n",
      "\n",
      "        prediction_error = update_values(position, chosen_action, new_position, outcome)\n",
      "        print \"iteration:\", actions_executed, \"action:\", chosen_action \n",
      "        print \"new pos:\", new_position, \"reward:\", outcome, \"updated values:\", values[position['x']][position['y']], \"prediction error:\", prediction_error\n",
      "\n",
      "        for i in range(num_actions):\n",
      "            nest.SetStatus(nest.GetConnections(states[position['x']][position['y']], actions[i]), {'weight': values[position['x']][position['y']][i] * WEIGHT_SCALING})\n",
      "            \n",
      "        # stimulate new state\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 0.})\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[new_position['x']][new_position['y']]), {'weight': 1.})\n",
      "\n",
      "        nest.SetStatus(wta_noise, {'rate': 0.})\n",
      "        nest.Simulate(50.)\n",
      "              \n",
      "        last_action_time += 250\n",
      "        actions_executed += 1\n",
      "    else:      \n",
      "        _, in_end_position = env.init_new_trial()\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 0.})\n",
      "\n",
      "\n",
      "rplt.from_device(sd_wta, title=\"WTA circuit\")\n",
      "rplt.from_device(sd_states, title=\"states\")\n",
      "rplt.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration: 0 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 action: 1\n",
        "new pos: {'y': 0, 'x': 2} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 action: 3\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 action: 1\n",
        "new pos: {'y': 0, 'x': 2} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 action: 3\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 action: 3\n",
        "new pos: {'y': 0, 'x': 0} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 action: 3\n",
        "new pos: {'y': 0, 'x': 0} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: -1 updated values: [-0.5  0.   0.   0. ] prediction error: -1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.5  0.   0.   0. ] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12 action: 3\n",
        "new pos: {'y': 1, 'x': 0} reward: -1 updated values: [ 0.   0.   0.  -0.5] prediction error: -1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.   0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14 action: 2\n",
        "new pos: {'y': 2, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15 action: 1\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.   0.5  0.   0. ] prediction error: 1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 16 action: 3\n",
        "new pos: {'y': 0, 'x': 0} reward: -1 updated values: [-0.5  0.   0.  -0.5] prediction error: -1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 17 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [-0.5  0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 19 action: 0\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 21 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 22 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.   0.   0.5  0. ] prediction error: 1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 23 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [-0.5  0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 24 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.  0.  0.  0.] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.   0.2  0.   0. ] prediction error: 0.4\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 26 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.    0.    0.75  0.  ] prediction error: 0.5\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 27 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.5  0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 28 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 0 updated values: [ 0.   0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 29 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [-0.5  0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 30 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.    0.    0.08  0.  ] prediction error: 0.16\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 31 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.   0.4  0.   0. ] prediction error: 0.4\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 32 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.     0.     0.875  0.   ] prediction error: 0.25\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 33 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.5  0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 34 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 0 updated values: [ 0.   0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 35 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: -1 updated values: [-0.75  0.    0.   -0.5 ] prediction error: -0.5\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 36 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75  0.    0.   -0.5 ] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 37 action: 2\n",
        "new pos: {'y': 2, 'x': 0} reward: 0 updated values: [ 0.   0.   0.  -0.5] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 38 action: 2\n",
        "new pos: {'y': 2, 'x': 0} reward: -1 updated values: [ 0.   0.  -0.5  0. ] prediction error: -1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 39 action: 3\n",
        "new pos: {'y': 2, 'x': 0} reward: -1 updated values: [ 0.   0.  -0.5 -0.5] prediction error: -1.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 40 action: 1\n",
        "new pos: {'y': 2, 'x': 1} reward: 0 updated values: [ 0.   0.2 -0.5 -0.5] prediction error: 0.4\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 41 action: 1\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.    0.75  0.    0.  ] prediction error: 0.5\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 42 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [-0.75   0.032  0.    -0.5  ] prediction error: 0.064\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 43 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.   0.   0.2  0. ] prediction error: 0.24\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 44 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.    0.55  0.    0.  ] prediction error: 0.3\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 45 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.      0.      0.9375  0.    ] prediction error: 0.125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 46 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75   0.032  0.    -0.5  ] prediction error: 0.0\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 47 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.    0.22  0.   -0.5 ] prediction error: 0.44\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 48 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.    0.65  0.    0.  ] prediction error: 0.2\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 49 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.       0.       0.96875  0.     ] prediction error: 0.0625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 50 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [-0.75   0.096  0.    -0.5  ] prediction error: 0.128\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 51 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.    0.    0.36  0.  ] prediction error: 0.32\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 52 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.      0.7125  0.      0.    ] prediction error: 0.125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 53 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.        0.        0.984375  0.      ] prediction error: 0.03125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 54 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75   0.096  0.088 -0.5  ] prediction error: 0.176\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 55 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.     0.395  0.    -0.5  ] prediction error: 0.35\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 56 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.    0.75  0.    0.  ] prediction error: 0.075\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 57 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.         0.         0.9921875  0.       ] prediction error: 0.015625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 58 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75   0.096  0.202 -0.5  ] prediction error: 0.228\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 59 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.      0.4975  0.     -0.5   ] prediction error: 0.205\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 60 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.        0.771875  0.        0.      ] prediction error: 0.04375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 61 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99609375  0.        ] prediction error: 0.0078125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 62 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75   0.096  0.3   -0.5  ] prediction error: 0.196\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 63 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.      0.5575  0.     -0.5   ] prediction error: 0.12\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 64 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.        0.784375  0.        0.      ] prediction error: 0.025\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 65 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99804688  0.        ] prediction error: 0.00390625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 66 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75   0.096  0.373 -0.5  ] prediction error: 0.146\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 67 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.      0.5925  0.     -0.5   ] prediction error: 0.07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 68 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79140625  0.          0.        ] prediction error: 0.0140625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 69 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99902344  0.        ] prediction error: 0.001953125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 70 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75    0.096   0.4235 -0.5   ] prediction error: 0.101\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 71 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.         0.6128125  0.        -0.5      ] prediction error: 0.040625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 72 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.         0.7953125  0.         0.       ] prediction error: 0.0078125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 73 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99951172  0.        ] prediction error: 0.0009765625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 74 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75      0.096     0.456875 -0.5     ] prediction error: 0.06675\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 75 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.62453125  0.         -0.5       ] prediction error: 0.0234375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 76 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79746094  0.          0.        ] prediction error: 0.004296875\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 77 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99975586  0.        ] prediction error: 0.00048828125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 78 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75     0.096    0.47825 -0.5    ] prediction error: 0.04275\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 79 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.       0.63125  0.      -0.5    ] prediction error: 0.0134375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 80 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79863281  0.          0.        ] prediction error: 0.00234375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 81 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99987793  0.        ] prediction error: 0.000244140625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 82 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75      0.096     0.491625 -0.5     ] prediction error: 0.02675\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 83 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63507813  0.         -0.5       ] prediction error: 0.00765625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 84 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79926758  0.          0.        ] prediction error: 0.00126953125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 85 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99993896  0.        ] prediction error: 0.0001220703125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 86 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.49984375 -0.5       ] prediction error: 0.0164375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 87 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63724609  0.         -0.5       ] prediction error: 0.0043359375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 88 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79960938  0.          0.        ] prediction error: 0.00068359375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 89 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99996948  0.        ] prediction error: 6.103515625e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 90 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.50482031 -0.5       ] prediction error: 0.009953125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 91 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.         0.6384668  0.        -0.5      ] prediction error: 0.00244140625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 92 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79979248  0.          0.        ] prediction error: 0.0003662109375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 93 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99998474  0.        ] prediction error: 3.0517578125e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 94 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.50779688 -0.5       ] prediction error: 0.005953125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 95 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63915039  0.         -0.5       ] prediction error: 0.0013671875\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 96 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79989014  0.          0.        ] prediction error: 0.0001953125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 97 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999237  0.        ] prediction error: 1.52587890625e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 98 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.50955859 -0.5       ] prediction error: 0.0035234375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 99 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63953125  0.         -0.5       ] prediction error: 0.00076171875\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79994202  0.          0.        ] prediction error: 0.000103759765625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 101 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999619  0.        ] prediction error: 7.62939453125e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 102 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75       0.096      0.5105918 -0.5      ] prediction error: 0.00206640625\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 103 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63974243  0.         -0.5       ] prediction error: 0.00042236328125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 104 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79996948  0.          0.        ] prediction error: 5.49316406251e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 105 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999809  0.        ] prediction error: 3.81469726562e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 106 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51119287 -0.5       ] prediction error: 0.0012021484375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 107 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63985901  0.         -0.5       ] prediction error: 0.000233154296875\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 108 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79998398  0.          0.        ] prediction error: 2.89916992188e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 109 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999905  0.        ] prediction error: 1.90734863281e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 110 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51154004 -0.5       ] prediction error: 0.0006943359375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 111 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.         0.6399231  0.        -0.5      ] prediction error: 0.000128173828125\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 112 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999161  0.          0.        ] prediction error: 1.52587890625e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 113 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999952  0.        ] prediction error: 9.53674316406e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 114 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51173926 -0.5       ] prediction error: 0.0003984375\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 115 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63995819  0.         -0.5       ] prediction error: 7.01904296876e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 116 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999561  0.          0.        ] prediction error: 8.01086425783e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 117 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999976  0.        ] prediction error: 4.76837158203e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 118 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51185291 -0.5       ] prediction error: 0.000227294921875\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 119 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63997734  0.         -0.5       ] prediction error: 3.82995605468e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 120 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999771  0.          0.        ] prediction error: 4.19616699232e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 121 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999988  0.        ] prediction error: 2.38418579102e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 122 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51191739 -0.5       ] prediction error: 0.000128967285156\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 123 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63998775  0.         -0.5       ] prediction error: 2.08282470704e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 124 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999881  0.          0.        ] prediction error: 2.19345092778e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 125 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999994  0.        ] prediction error: 1.19209289551e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 126 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75       0.096      0.5119538 -0.5      ] prediction error: 7.28149414063e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 127 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.         0.6399934  0.        -0.5      ] prediction error: 1.12915039063e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 128 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999938  0.          0.        ] prediction error: 1.14440917964e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 129 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999997  0.        ] prediction error: 5.96046447754e-08\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 130 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51197426 -0.5       ] prediction error: 4.09240722657e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 131 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63999645  0.         -0.5       ] prediction error: 6.10351562502e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 132 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999968  0.          0.        ] prediction error: 5.96046447754e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 133 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999999  0.        ] prediction error: 2.98023223877e-08\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 134 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51198571 -0.5       ] prediction error: 2.2903442383e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 135 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.         0.6399981  0.        -0.5      ] prediction error: 3.29017639156e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 136 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999983  0.          0.        ] prediction error: 3.09944152899e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 137 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.          0.          0.99999999  0.        ] prediction error: 1.49011611938e-08\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 138 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51199209 -0.5       ] prediction error: 1.27677917481e-05\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 139 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63999898  0.         -0.5       ] prediction error: 1.7690658568e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 140 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999991  0.          0.        ] prediction error: 1.6093254096e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 141 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.  0.  1.  0.] prediction error: 7.45058059692e-09\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 142 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.096       0.51199564 -0.5       ] prediction error: 7.09152221678e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 143 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63999946  0.         -0.5       ] prediction error: 9.48905944931e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 144 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999996  0.          0.        ] prediction error: 8.34465025967e-08\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 145 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.  0.  1.  0.] prediction error: 3.72529029846e-09\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 146 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [-0.75        0.192       0.51199564 -0.5       ] prediction error: 0.192\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 147 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.          0.49999998  0.        ] prediction error: 0.279999964237\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 148 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999998  0.          0.        ] prediction error: 4.321336744e-08\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 149 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.  0.  1.  0.] prediction error: 1.86264514923e-09\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 150 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75       0.192      0.5119976 -0.5      ] prediction error: 3.92532348636e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 151 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63999972  0.         -0.5       ] prediction error: 5.25116920502e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 152 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999999  0.          0.        ] prediction error: 2.23517417908e-08\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 153 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.  0.  1.  0.] prediction error: 9.31322574615e-10\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 154 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.192       0.51199869 -0.5       ] prediction error: 2.17270851144e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 155 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63999985  0.         -0.5       ] prediction error: 2.71499156934e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 156 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.          0.79999999  0.          0.        ] prediction error: 1.1548399903e-08\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 157 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.  0.  1.  0.] prediction error: 4.65661287308e-10\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 158 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [-0.75        0.29599999  0.51199869 -0.5       ] prediction error: 0.207999985695\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 159 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.          0.56999999  0.        ] prediction error: 0.140000012964\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 160 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.   0.8  0.   0. ] prediction error: 5.96046434431e-09\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 161 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.  0.  1.  0.] prediction error: 2.32830643654e-10\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 162 action: 1\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 updated values: [-0.75        0.37599999  0.51199869 -0.5       ] prediction error: 0.159999998033\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 163 action: 2\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.          0.60499999  0.        ] prediction error: 0.0700000088662\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 164 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.   0.8  0.   0. ] prediction error: 3.07336445182e-09\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 165 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.  0.  1.  0.] prediction error: 1.16415321827e-10\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 166 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.37599999  0.51199929 -0.5       ] prediction error: 1.19495391859e-06\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 167 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63999993  0.         -0.5       ] prediction error: 1.43982469947e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 168 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.   0.8  0.   0. ] prediction error: 1.58324842126e-09\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 169 action: 2\n",
        "new pos: {'y': 2, 'x': 2} reward: 1.0 updated values: [ 0.  0.  1.  0.] prediction error: 5.82076609135e-11\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 170 action: 2\n",
        "new pos: {'y': 1, 'x': 0} reward: 0 updated values: [-0.75        0.37599999  0.51199961 -0.5       ] prediction error: 6.55069947286e-07\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 171 action: 1\n",
        "new pos: {'y': 1, 'x': 1} reward: 0 updated values: [ 0.          0.63999996  0.         -0.5       ] prediction error: 7.26245343863e-08\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 172 action: 1\n",
        "new pos: {'y': 1, 'x': 2} reward: 0 updated values: [ 0.   0.8  0.   0. ] prediction error: 8.14907252789e-10\n",
        "iteration:"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}