{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Artificial Predicion Error"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Learning from reward and punishment driven by prediction error signal \u2013 artificial non-neural implementation."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We saw now that WTA dynamics can be interpreted as action selection and tuning the weights can influence which actions are selected in certain given states. Now how can we let the neural circuit tune these weights from positive or negative experience made in a task, so that it learns to select those actions that lead to rewards and to avoid those actions that lead to punishment?\n",
      "\n",
      "For that we need to define a learning or update rule that changes the weights in reasonable way. We also need to define what does it mean to obtain reward or to get a punishment.\n",
      "\n",
      "To demonstrate how this may work in a simple setting, we install an environment called \u201cgrid world\u201d. It has discrete  state an agent controlled by the neural network can be in, and actions that can be executed in each state that moves the agent from one grid square field to another. So it is a very simple version of a labyrinth task. \n",
      "\n",
      "If we define some of the grids of the grid world to contain a reward, we can provide the network with a reward signal. The same can be done for punishment.\n",
      "\n",
      "In frame of TD learning that we had introduced in the lecture, we need to keep track of so called outcome expectations, or Q-values, that has to be update each time the network experiences something it has not been able to properly predict. These updates are driven by a so called prediction error signal that is generated each time the network encounters an unpredicted reward or punishment."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1.) 1 State - 2 Actions"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We work now on a very simple grid world instance consisting only of three states \u2013 the starting state and two states that the agent can reach from the start by executing one of two possible actions, move left or move right. This is equivalent to forced binary choice task, where a decision between two alternatives has to be made. One of the states will provide reward, the other state offers nothing. In spirit of a particular TD learning instantiation called Q-learning, create an array containing a Q(s,a) value for each state-action, or in other words, for each synaptic connection from a state pool to an action pool. Implement the equation to compute prediction error given the reward signal and the Q values. Is there a way of making the prediction error equation for this particular binary choice task even more simple than its full form? Implement update equation that changes the Q values based on the computed prediction error. How can the Q values be used now to make WTA circuit do the proper decisions about the actions to select given the state signals? Implement the necessary operation that utilizes the Q values in the suitable way."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import nest\n",
      "import nest.raster_plot as rplt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import environment as env"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "As in the exercise before, we set up a simple environment and create a WTA circuit to choose actions within this environment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "env.set_environment(0)\n",
      "\n",
      "\n",
      "NUM_ITERATIONS = 50\n",
      "LEARNING_RATE = 0.05\n",
      "NUM_STATE_NEURONS = 20\n",
      "NUM_WTA_NEURONS = 50\n",
      "WEIGHT_SCALING = 100 / NUM_STATE_NEURONS\n",
      "\n",
      "nest.ResetKernel()\n",
      "nest.set_verbosity(\"M_DEBUG\")\n",
      "\n",
      "rank = nest.Rank()\n",
      "size = nest.NumProcesses() \n",
      "seed = np.random.randint(0, 1000000)\n",
      "num_threads = 4\n",
      "nest.SetKernelStatus({\"local_num_threads\": num_threads})\n",
      "nest.SetKernelStatus({\"rng_seeds\": range(seed+num_threads * size + 1, seed + 2 * (num_threads * size) + 1),\n",
      "        \t\t      \"grng_seed\": seed+size+num_threads,\n",
      "                      \"resolution\": 0.1})\n",
      "\n",
      "# Create states\n",
      "world_dim = env.get_world_dimensions()\n",
      "states = []\n",
      "for i in range(world_dim['x']):\n",
      "    states.append(nest.Create('iaf_psc_alpha', NUM_STATE_NEURONS))\n",
      "all_states = np.ravel(states).tolist()\n",
      "\n",
      "# Create actions\n",
      "num_actions = env.get_num_possible_actions()\n",
      "actions = []\n",
      "for i in range(num_actions):\n",
      "    actions.append(nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS))\n",
      "all_actions = np.ravel(actions).tolist()\n",
      "\n",
      "# Create WTA circuit\n",
      "wta_ex_weights = 10.5\n",
      "wta_inh_weights = -2.6\n",
      "wta_ex_inh_weights = 2.8\n",
      "wta_noise_weights = 2.1\n",
      "\n",
      "wta_inh_neurons = nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS)\n",
      "\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], actions[i], 'all_to_all', {'weight': wta_ex_weights})\n",
      "    nest.Connect(actions[i], wta_inh_neurons, 'all_to_all', {'weight': wta_ex_inh_weights}) \n",
      "\n",
      "nest.Connect(wta_inh_neurons, all_actions, 'all_to_all', {'weight': wta_inh_weights})\n",
      "\n",
      "wta_noise = nest.Create('poisson_generator', 10, {'rate': 3000.})\n",
      "nest.Connect(wta_noise, all_actions, 'all_to_all', {'weight': wta_noise_weights})\n",
      "nest.Connect(wta_noise, wta_inh_neurons, 'all_to_all', {'weight': wta_noise_weights * 0.9})\n",
      "\n",
      "# Create noise\n",
      "noise = nest.Create('poisson_generator', 1, {'rate': 65000.})\n",
      "nest.Connect(noise, all_states, 'all_to_all', {'weight': 1.})\n",
      "\n",
      "# Create stimulus\n",
      "stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "position = env.get_agent_pos()['x']\n",
      "nest.Connect(stimulus, states[position], 'all_to_all', {'weight': 1.})\n",
      "\n",
      "# Create spike detector\n",
      "sd_wta = nest.Create('spike_detector')\n",
      "nest.Connect(all_actions, sd_wta)\n",
      "sd_actions = nest.Create('spike_detector', num_actions)\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], [sd_actions[i]])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This time we estimate the valence of each action in 'values' and connect the state population to the action populations accordingly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Value expectations for Q(s,a)\n",
      "values = np.zeros(num_actions)\n",
      "    \n",
      "# Connect states to actions\n",
      "nest.Connect(states[1], actions[0], 'all_to_all', {'weight': values[0]})\n",
      "nest.Connect(states[1], actions[1], 'all_to_all', {'weight': values[1]})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "After each selected action, this function is called which will update the valence of the state."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_values(chosen_action, outcome):\n",
      "    # prediction error\n",
      "    prediction_error = outcome - values[chosen_action]\n",
      "\n",
      "    # update values\n",
      "    values[chosen_action] += prediction_error * LEARNING_RATE  \n",
      "\n",
      "    # save values for plotting\n",
      "    values_hist.append(values.copy())\n",
      "    return prediction_error\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "And finally we run the simulation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Main loop\n",
      "values_hist = [values.copy()]\n",
      "actions_executed = 0\n",
      "last_action_time = 0\n",
      "in_end_position = False\n",
      "\n",
      "while actions_executed < NUM_ITERATIONS:\n",
      "    if not in_end_position:\n",
      "        nest.SetStatus(wta_noise, {'rate': 3000.})\n",
      "        nest.Simulate(200)\n",
      "        max_rate = -1\n",
      "        chosen_action = -1\n",
      "        for i in range(len(sd_actions)):\n",
      "            rate = len([e for e in nest.GetStatus([sd_actions[i]], keys='events')[0]['times'] if e > last_action_time]) # calc the \"firerate\" of each actor population\n",
      "            if rate > max_rate:\n",
      "                max_rate = rate # the population with the hightes rate wins\n",
      "                chosen_action = i\n",
      "        nest.SetStatus(stimulus, {'rate': 5000.})\n",
      "\n",
      "        possible_actions = env.get_possible_actions() \n",
      "\n",
      "        _, outcome, in_end_position = env.move(possible_actions[chosen_action])\n",
      "\n",
      "        prediction_error = update_values(chosen_action, outcome)\n",
      "        print \"iteration:\", actions_executed, \"action:\", chosen_action, \"reward:\", outcome, \"updated values\", values, \"predicion error:\", prediction_error\n",
      "\n",
      "\n",
      "        for i in range(num_actions):\n",
      "            nest.SetStatus(nest.GetConnections(states[1], actions[i]), {'weight': values[i] * WEIGHT_SCALING})\n",
      "\n",
      "        nest.SetStatus(wta_noise, {'rate': 0.})\n",
      "        nest.Simulate(50.)\n",
      "\n",
      "        last_action_time += 1000\n",
      "        actions_executed += 1\n",
      "    else:\n",
      "        _, in_end_position = env.init_new_trial()\n",
      "\n",
      "       \n",
      "plt.xlabel(\"# action\")\n",
      "plt.ylabel(\"valence\")\n",
      "plt.title(\"valence of each action\")\n",
      "plt.plot(values_hist)\n",
      "rplt.from_device(sd_wta, title=\"WTA circuit\")\n",
      "rplt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise:\n",
      "\n",
      "How can the learning progress be visualized? What do you expected the Q values and prediction error to evolve like in course of learning? Provide plots that give insight about the learning progress. Experiment around with providing different reward magnitudes or having a punishing state instead of a neutral one. Describe you observations. Are there any issues with learning from punishment? If yes, is there any workaround you can think of? If there are no issues \u2013 fine. ))\n",
      "\n",
      "Change the iterations and learning rate. What is the pro and contra of high learning rates?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2.) Multi State - 2 Actions"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "!!! PLEASE RESTART KERNEL HERE !!!\n",
      "\n",
      "In this example show how to handle multiple states.\n",
      "\n",
      "We extend the world to have more states than only three. The intermediate states has no consequence, the two final states \u2013 most left and most right \u2013 can be chosen to provide a reward and punishment or reward and nothing. \n",
      "\n",
      "Exercise: What is the crucial different between the task where the consequence \u2013 reward or punishment \u2013 is provided immediately after executing an action and the task where the consequence is delayed? Do the same kind of analysis on the learning progress providing the plots."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nest\n",
      "import nest.raster_plot as rplt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib as mpl\n",
      "import environment as env\n",
      "from mpl_toolkits.mplot3d import Axes3D"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "env.set_environment(1)\n",
      "\n",
      "\n",
      "NUM_ITERATIONS = 30\n",
      "LEARNING_RATE = 0.5\n",
      "NUM_STATE_NEURONS = 20\n",
      "NUM_WTA_NEURONS = 50\n",
      "WEIGHT_SCALING = 100 / NUM_STATE_NEURONS\n",
      "\n",
      "nest.ResetKernel()\n",
      "nest.set_verbosity(\"M_DEBUG\")\n",
      "\n",
      "rank = nest.Rank()\n",
      "size = nest.NumProcesses() \n",
      "seed = np.random.randint(0, 1000000)\n",
      "num_threads = 4\n",
      "nest.SetKernelStatus({\"local_num_threads\": num_threads})\n",
      "nest.SetKernelStatus({\"rng_seeds\": range(seed+num_threads * size + 1, seed + 2 * (num_threads * size) + 1),\n",
      "        \t\t      \"grng_seed\": seed+size+num_threads,\n",
      "                      \"resolution\": 0.1})\n",
      "\n",
      "\n",
      "# Create states\n",
      "world_dim = env.get_world_dimensions()\n",
      "states = []\n",
      "for i in range(world_dim['x']):\n",
      "    states.append(nest.Create('iaf_psc_alpha', NUM_STATE_NEURONS))\n",
      "all_states = np.ravel(states).tolist()\n",
      "\n",
      "# Create actions\n",
      "num_actions = env.get_num_possible_actions()\n",
      "actions = []\n",
      "for i in range(num_actions):\n",
      "    actions.append(nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS))\n",
      "all_actions = np.ravel(actions).tolist()\n",
      "\n",
      "# Create WTA circuit\n",
      "wta_ex_weights = 10.5\n",
      "wta_inh_weights = -2.6\n",
      "wta_ex_inh_weights = 2.8\n",
      "wta_noise_weights = 2.1\n",
      "\n",
      "wta_inh_neurons = nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS)\n",
      "\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], actions[i], 'all_to_all', {'weight': wta_ex_weights})\n",
      "    nest.Connect(actions[i], wta_inh_neurons, 'all_to_all', {'weight': wta_ex_inh_weights}) \n",
      "\n",
      "nest.Connect(wta_inh_neurons, all_actions, 'all_to_all', {'weight': wta_inh_weights})\n",
      "\n",
      "wta_noise = nest.Create('poisson_generator', 10, {'rate': 3000.})\n",
      "nest.Connect(wta_noise, all_actions, 'all_to_all', {'weight': wta_noise_weights})\n",
      "nest.Connect(wta_noise, wta_inh_neurons, 'all_to_all', {'weight': wta_noise_weights * 0.9})\n",
      "\n",
      "# Create noise\n",
      "noise = nest.Create('poisson_generator', 1, {'rate': 65000.})\n",
      "nest.Connect(noise, all_states, 'all_to_all', {'weight': 1.})\n",
      "\n",
      "# Create stimulus\n",
      "stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "nest.Connect(stimulus, all_states, 'all_to_all', {'weight': 0.})\n",
      "\n",
      "# Create spike detector\n",
      "sd_wta = nest.Create('spike_detector')\n",
      "nest.Connect(all_actions, sd_wta)\n",
      "sd_actions = nest.Create('spike_detector', num_actions)\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], [sd_actions[i]])\n",
      "sd_states = nest.Create('spike_detector')\n",
      "nest.Connect(all_states, sd_states)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Create values for each action in each state of the environment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Value expectations for Q(s,a)\n",
      "values = np.array([np.zeros(num_actions) for i in range(world_dim['x'])])\n",
      "        \n",
      "# Connect states to actions with initial weight 0.0\n",
      "nest.Connect(all_states, all_actions, 'all_to_all', {'weight': 0.0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gamma = 0.8\n",
      "\n",
      "def update_values(position, chosen_action, new_position, outcome):\n",
      "    # prediction error\n",
      "    best_new_action = values[new_position].argmax()\n",
      "    prediction_error = outcome + gamma * values[new_position][best_new_action] - values[position][chosen_action]\n",
      "\n",
      "    # update values\n",
      "    values[position][chosen_action] += prediction_error * LEARNING_RATE \n",
      "\n",
      "    # save values for plotting\n",
      "    values_hist.append(np.ravel(values.copy()))\n",
      "    \n",
      "    return prediction_error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_values(fig, ax, position):\n",
      "    plt.cla()\n",
      "    \n",
      "    values_plot = [[]]\n",
      "\n",
      "    \n",
      "    for i in range(len(states)):\n",
      "        values_plot[0].append(max(values[i]))\n",
      "    \n",
      "    xlabels = np.arange(0, len(states))\n",
      "    ylabels = np.arange(0, len(states[0]))\n",
      "\n",
      "    # Set the major ticks at the centers and minor tick at the edges\n",
      "    xlocs = np.arange(len(xlabels))\n",
      "    ylocs = np.arange(len(ylabels))\n",
      "    ax.xaxis.set_ticks(xlocs + 0.5, minor=True)\n",
      "    ax.xaxis.set(ticks=xlocs, ticklabels=xlabels)\n",
      "    ax.yaxis.set_ticks(ylocs + 0.5, minor=True)\n",
      "    ax.yaxis.set(ticks=ylocs, ticklabels=ylabels)\n",
      "    \n",
      "    # Turn on the grid for the minor ticks\n",
      "    ax.grid(True, which='minor', linestyle='-', linewidth=2)   \n",
      "    plt.imshow(values_plot, interpolation='None', vmin=-1, vmax=1)\n",
      "    \n",
      "    for txt in ax.texts:\n",
      "        txt.set_visible(False)\n",
      "        \n",
      "    ax.annotate(\".\", ((position['x'] + 0.5)/len(states), 0.5), size=160, textcoords='axes fraction')\n",
      "    plt.draw()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Main loop\n",
      "values_hist = [np.ravel(values.copy())]\n",
      "actions_executed = 0\n",
      "last_action_time = 0\n",
      "in_end_position = False\n",
      "\n",
      "# interactive plotting\n",
      "fig, ax = plt.subplots()\n",
      "plt.ion()\n",
      "\n",
      "\n",
      "\n",
      "while actions_executed < NUM_ITERATIONS:\n",
      "    position = env.get_agent_pos().copy()    \n",
      "    \n",
      "    # plotting\n",
      "    plot_values(fig, ax, position)\n",
      "    \n",
      "    if not in_end_position:        \n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']]), {'weight': 1.})\n",
      "        \n",
      "        nest.SetStatus(wta_noise, {'rate': 3000.})\n",
      "        nest.Simulate(200)\n",
      "        max_rate = -1\n",
      "        chosen_action = -1\n",
      "        for i in range(len(sd_actions)):\n",
      "            rate = len([e for e in nest.GetStatus([sd_actions[i]], keys='events')[0]['times'] if e > last_action_time]) # calc the \"firerate\" of each actor population\n",
      "            if rate > max_rate:\n",
      "                max_rate = rate # the population with the hightes rate wins\n",
      "                chosen_action = i\n",
      "        nest.SetStatus(stimulus, {'rate': 5000.})\n",
      "\n",
      "        possible_actions = env.get_possible_actions() \n",
      "\n",
      "        new_position, outcome, in_end_position = env.move(possible_actions[chosen_action])\n",
      "\n",
      "        prediction_error = update_values(position['x'], chosen_action, new_position['x'], outcome)\n",
      "        \n",
      "        print \"iteration:\", actions_executed, \"action:\", chosen_action\n",
      "        print \"new position:\", new_position['x'], \"reward:\", outcome, \"updated_values:\", values[position['x']], \"prediction error:\", prediction_error\n",
      "\n",
      "        for i in range(num_actions):\n",
      "            nest.SetStatus(nest.GetConnections(states[position['x']], actions[i]), {'weight': values[position['x']][i] * WEIGHT_SCALING})\n",
      "            \n",
      "        # stimulate new state\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']]), {'weight': 0.})\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[new_position['x']]), {'weight': 1.})\n",
      "\n",
      "        nest.SetStatus(wta_noise, {'rate': 0.})\n",
      "        nest.Simulate(50.)\n",
      "\n",
      "        last_action_time += 250\n",
      "        actions_executed += 1\n",
      "    else:\n",
      "        _, in_end_position = env.init_new_trial()\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']]), {'weight': 0.})\n",
      "        \n",
      "\n",
      "    \n",
      "   \n",
      "#rplt.from_device(sd_wta, title=\"WTA circuit\")\n",
      "#rplt.from_device(sd_states, title=\"states\")\n",
      "#rplt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3.) Grid World --- Multiple States - 4 Actions"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "!!! PLEASE RESTART KERNEL HERE !!!\n",
      "\n",
      "Exercise : We extend the world to have a 2D grid of NxN size. We have more possible actions now for each state \u2013 4 instead of 2 (down, up, left, right). Do the same kind of analysis on the learning progress providing the plots."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nest\n",
      "import nest.raster_plot as rplt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import environment as env\n",
      "from mpl_toolkits.mplot3d import Axes3D"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "env.set_environment(7)\n",
      "\n",
      "NUM_ITERATIONS = 2000\n",
      "LEARNING_RATE = 0.5\n",
      "NUM_STATE_NEURONS = 20\n",
      "NUM_WTA_NEURONS = 50\n",
      "WEIGHT_SCALING = 100 / NUM_STATE_NEURONS\n",
      "\n",
      "nest.ResetKernel()\n",
      "nest.set_verbosity(\"M_DEBUG\")\n",
      "\n",
      "rank = nest.Rank()\n",
      "size = nest.NumProcesses() \n",
      "seed = np.random.randint(0, 1000000)\n",
      "num_threads = 4\n",
      "nest.SetKernelStatus({\"local_num_threads\": num_threads})\n",
      "nest.SetKernelStatus({\"rng_seeds\": range(seed+num_threads * size + 1, seed + 2 * (num_threads * size) + 1),\n",
      "        \t\t      \"grng_seed\": seed+size+num_threads,\n",
      "                      \"resolution\": 0.1})\n",
      "\n",
      "# Create states\n",
      "world_dim = env.get_world_dimensions()\n",
      "states = []\n",
      "for i in range(world_dim['x']):\n",
      "    states.append([])\n",
      "    for j in range(world_dim['y']):\n",
      "        states[i].append(nest.Create('iaf_psc_alpha', NUM_STATE_NEURONS))\n",
      "all_states = np.ravel(states).tolist()\n",
      "\n",
      "# Create actions\n",
      "num_actions = env.get_num_possible_actions()\n",
      "actions = []\n",
      "for i in range(num_actions):\n",
      "    actions.append(nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS))\n",
      "all_actions = np.ravel(actions).tolist()\n",
      "\n",
      "# Create WTA circuit\n",
      "wta_ex_weights = 10.5\n",
      "wta_inh_weights = -2.6\n",
      "wta_ex_inh_weights = 2.8\n",
      "wta_noise_weights = 2.1\n",
      "\n",
      "wta_inh_neurons = nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS)\n",
      "\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], actions[i], 'all_to_all', {'weight': wta_ex_weights})\n",
      "    nest.Connect(actions[i], wta_inh_neurons, 'all_to_all', {'weight': wta_ex_inh_weights}) \n",
      "\n",
      "nest.Connect(wta_inh_neurons, all_actions, 'all_to_all', {'weight': wta_inh_weights})\n",
      "\n",
      "wta_noise = nest.Create('poisson_generator', 10, {'rate': 3000.})\n",
      "nest.Connect(wta_noise, all_actions, 'all_to_all', {'weight': wta_noise_weights})\n",
      "nest.Connect(wta_noise, wta_inh_neurons, 'all_to_all', {'weight': wta_noise_weights * 0.9})\n",
      "\n",
      "# Create noise\n",
      "noise = nest.Create('poisson_generator', 1, {'rate': 65000.})\n",
      "nest.Connect(noise, all_states, 'all_to_all', {'weight': 1.})\n",
      "\n",
      "# Create stimulus\n",
      "stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "nest.Connect(stimulus, all_states, 'all_to_all', {'weight': 0.})\n",
      "\n",
      "# Create spike detector\n",
      "sd_wta = nest.Create('spike_detector')\n",
      "nest.Connect(all_actions, sd_wta)\n",
      "nest.Connect(wta_inh_neurons, sd_wta)\n",
      "sd_actions = nest.Create('spike_detector', num_actions)\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], [sd_actions[i]])\n",
      "sd_states = nest.Create('spike_detector')\n",
      "nest.Connect(all_states, sd_states)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "values = []\n",
      "# Value expectations for Q(s,a)\n",
      "for i in range(world_dim['x']):\n",
      "    values.append([])\n",
      "    for j in range(world_dim['y']):\n",
      "        values[i].append(np.zeros(num_actions))\n",
      "        \n",
      "# Connect states to actions with initial weight 0.0\n",
      "nest.Connect(all_states, all_actions, 'all_to_all', {'weight': 0.0})\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gamma = 0.8\n",
      "\n",
      "def update_values(position, chosen_action, new_position, outcome):\n",
      "    # prediction error\n",
      "    best_new_action = values[new_position['x']][new_position['y']].argmax()\n",
      "    prediction_error = outcome + gamma * values[new_position['x']][new_position['y']][best_new_action] - values[position['x']][position['y']][chosen_action]\n",
      "    \n",
      "    # update values\n",
      "    values[position['x']][position['y']][chosen_action] += prediction_error * LEARNING_RATE \n",
      "    return prediction_error\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_values(fig, ax, position):\n",
      "    plt.cla()\n",
      "    \n",
      "    values_plot = []      \n",
      "    \n",
      "    for i in range(world_dim['y']):\n",
      "        values_plot.append([])\n",
      "        for j in range(world_dim['x']):\n",
      "            values_plot[i].append(np.max(values[j][i]))\n",
      "    \n",
      "    values_plot = np.array(values_plot)\n",
      "    \n",
      "    plt.imshow(values_plot, interpolation='none', vmax= 1, vmin = -1)\n",
      "    \n",
      "    xlabels = np.arange(0, len(states))\n",
      "    ylabels = np.arange(0, len(states[0]))\n",
      "\n",
      "    # Set the major ticks at the centers and minor tick at the edges\n",
      "    xlocs = np.arange(len(xlabels))\n",
      "    ylocs = np.arange(len(ylabels))\n",
      "    ax.xaxis.set_ticks(xlocs + 0.5, minor=True)\n",
      "    ax.xaxis.set(ticks=xlocs, ticklabels=xlabels)\n",
      "    ax.yaxis.set_ticks(ylocs + 0.5, minor=True)\n",
      "    ax.yaxis.set(ticks=ylocs, ticklabels=ylabels)\n",
      "    \n",
      "    # Turn on the grid for the minor ticks\n",
      "    ax.grid(True, which='minor', linestyle='-', linewidth=2)   \n",
      "    \n",
      "    for txt in ax.texts:\n",
      "        txt.set_visible(False)\n",
      "        \n",
      "    ax.annotate(\".\", ((position['x'] + 0.5)/len(states), (1-(position['y'] + 0.5)/len(states[0]))), size=160, textcoords='axes fraction', color='white')\n",
      "    plt.draw()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Main loop\n",
      "actions_executed = 0\n",
      "last_action_time = 0\n",
      "in_end_position = False\n",
      "\n",
      "# interactive plotting\n",
      "fig, ax = plt.subplots()\n",
      "plt.ion()\n",
      "    \n",
      "while actions_executed < NUM_ITERATIONS:\n",
      "    position = env.get_agent_pos().copy()\n",
      "    # plotting\n",
      "    plot_values(fig, ax, position)\n",
      "    \n",
      "    if not in_end_position:        \n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 1.})\n",
      "        \n",
      "        nest.SetStatus(wta_noise, {'rate': 3000.})\n",
      "        nest.Simulate(200)\n",
      "        max_rate = -1\n",
      "        chosen_action = -1\n",
      "        for i in range(len(sd_actions)):\n",
      "            rate = len([e for e in nest.GetStatus([sd_actions[i]], keys='events')[0]['times'] if e > last_action_time]) # calc the \"firerate\" of each actor population\n",
      "            if rate > max_rate:\n",
      "                max_rate = rate # the population with the hightes rate wins\n",
      "                chosen_action = i\n",
      "        nest.SetStatus(stimulus, {'rate': 5000.})\n",
      "\n",
      "        possible_actions = env.get_possible_actions() \n",
      "\n",
      "        new_position, outcome, in_end_position = env.move(possible_actions[chosen_action])\n",
      "\n",
      "        prediction_error = update_values(position, chosen_action, new_position, outcome)\n",
      "        print \"iteration:\", actions_executed, \"action:\", chosen_action \n",
      "        print \"new pos:\", new_position, \"reward:\", outcome, \"updated values:\", values[position['x']][position['y']], \"prediction error:\", prediction_error\n",
      "\n",
      "        for i in range(num_actions):\n",
      "            nest.SetStatus(nest.GetConnections(states[position['x']][position['y']], actions[i]), {'weight': values[position['x']][position['y']][i] * WEIGHT_SCALING})\n",
      "            \n",
      "        # stimulate new state\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 0.})\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[new_position['x']][new_position['y']]), {'weight': 1.})\n",
      "\n",
      "        nest.SetStatus(wta_noise, {'rate': 0.})\n",
      "        nest.Simulate(50.)\n",
      "              \n",
      "        last_action_time += 250\n",
      "        actions_executed += 1\n",
      "    else:      \n",
      "        _, in_end_position = env.init_new_trial()\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 0.})\n",
      "\n",
      "\n",
      "rplt.from_device(sd_wta, title=\"WTA circuit\")\n",
      "rplt.from_device(sd_states, title=\"states\")\n",
      "rplt.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}