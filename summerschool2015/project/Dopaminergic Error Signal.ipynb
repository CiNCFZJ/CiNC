{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dopaminergic Error Signal"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "In this notebook, we will introduce a more plausible way to calculate the prediction error signal for reinforcement learning."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We start with the same implementation as before but without calculating the values (and weights from states to actions) explicitly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nest\n",
      "import nest.raster_plot as rplt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import environment as env\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "env.set_environment(0)\n",
      "\n",
      "NUM_ITERATIONS = 2000\n",
      "LEARNING_RATE = 0.5\n",
      "NUM_STATE_NEURONS = 20\n",
      "NUM_WTA_NEURONS = 50\n",
      "WEIGHT_SCALING = 100 / NUM_STATE_NEURONS\n",
      "\n",
      "nest.ResetKernel()\n",
      "nest.set_verbosity(\"M_DEBUG\")\n",
      "\n",
      "rank = nest.Rank()\n",
      "size = nest.NumProcesses() \n",
      "seed = np.random.randint(0, 1000000)\n",
      "num_threads = 1\n",
      "nest.SetKernelStatus({\"local_num_threads\": num_threads})\n",
      "nest.SetKernelStatus({\"rng_seeds\": range(seed+num_threads * size + 1, seed + 2 * (num_threads * size) + 1),\n",
      "        \t\t      \"grng_seed\": seed+size+num_threads,\n",
      "                      \"resolution\": 0.1})\n",
      "\n",
      "# Create states\n",
      "world_dim = env.get_world_dimensions()\n",
      "states = []\n",
      "for i in range(world_dim['x']):\n",
      "    states.append([])\n",
      "    for j in range(world_dim['y']):\n",
      "        states[i].append(nest.Create('iaf_psc_alpha', NUM_STATE_NEURONS))\n",
      "all_states = np.ravel(states).tolist()\n",
      "\n",
      "# Create actions\n",
      "num_actions = env.get_num_possible_actions()[0][1]\n",
      "actions = []\n",
      "for i in range(num_actions):\n",
      "    actions.append(nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS))\n",
      "all_actions = np.ravel(actions).tolist()\n",
      "\n",
      "# Create WTA circuit\n",
      "wta_ex_weights = 10.5\n",
      "wta_inh_weights = -2.6\n",
      "wta_ex_inh_weights = 2.8\n",
      "wta_noise_weights = 2.1\n",
      "\n",
      "wta_inh_neurons = nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS)\n",
      "\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], actions[i], 'all_to_all', {'weight': wta_ex_weights})\n",
      "    nest.Connect(actions[i], wta_inh_neurons, 'all_to_all', {'weight': wta_ex_inh_weights}) \n",
      "\n",
      "nest.Connect(wta_inh_neurons, all_actions, 'all_to_all', {'weight': wta_inh_weights})\n",
      "\n",
      "wta_noise = nest.Create('poisson_generator', 10, {'rate': 3000.})\n",
      "nest.Connect(wta_noise, all_actions, 'all_to_all', {'weight': wta_noise_weights})\n",
      "nest.Connect(wta_noise, wta_inh_neurons, 'all_to_all', {'weight': wta_noise_weights * 0.9})\n",
      "\n",
      "\n",
      "# Create stimulus\n",
      "stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "nest.Connect(stimulus, all_states, 'all_to_all', {'weight': 0.})\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Here, we are implementing the dopaminergic nueron pool, volume transmitter and dopamin-modulated synapse between states and actions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create DA pool\n",
      "DA_neurons = nest.Create('iaf_psc_alpha', 100)\n",
      "vol_trans = nest.Create('volume_transmitter', 1, {'deliver_interval': 10})\n",
      "nest.Connect(DA_neurons, vol_trans, 'all_to_all')\n",
      "\n",
      "# Create reward stimulus\n",
      "reward_stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "nest.Connect(reward_stimulus, DA_neurons, 'all_to_all', {'weight': 0.})\n",
      "\n",
      "tau_c = 500.0\n",
      "tau_n = 20.0\n",
      "\n",
      "# Connect states to actions\n",
      "nest.CopyModel('neuromod_dopa_synapse', 'dopa_synapse', {'vt': vol_trans[0], 'A_LTP': 400., 'A_LTD': 500.,\n",
      "                                                         'Wmin': -30., 'Wmax': 30., 'b': .1, 'tau_n': tau_n, 'tau_c': tau_c})\n",
      "        \n",
      "nest.Connect(all_states, all_actions, 'all_to_all', {'model': 'dopa_synapse', 'weight': 0.0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create spike detector\n",
      "sd_wta = nest.Create('spike_detector')\n",
      "nest.Connect(all_actions, sd_wta)\n",
      "nest.Connect(wta_inh_neurons, sd_wta)\n",
      "sd_actions = nest.Create('spike_detector', num_actions)\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], [sd_actions[i]])\n",
      "sd_states = nest.Create('spike_detector')\n",
      "nest.Connect(all_states, sd_states)\n",
      "sd_DA = nest.Create('spike_detector', 1)\n",
      "nest.Connect(DA_neurons, sd_DA, 'all_to_all')\n",
      "\n",
      "\n",
      "# Create noise\n",
      "noise = nest.Create('poisson_generator', 1, {'rate': 65000.})\n",
      "nest.Connect(noise, all_states, 'all_to_all', {'weight': 1.})\n",
      "nest.Connect(noise, DA_neurons, 'all_to_all', {'weight': 1.})\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_values(fig, ax, position):\n",
      "    plt.cla()\n",
      "    \n",
      "    values_plot = []      \n",
      "    \n",
      "    for i in range(world_dim['y']):\n",
      "        values_plot.append([])\n",
      "        for j in range(world_dim['x']):\n",
      "            values_plot[i].append(np.max([nest.GetStatus(nest.GetConnections(states[j][i], a), 'weight') for a in actions]))\n",
      "    \n",
      "    values_plot = np.array(values_plot)\n",
      "    \n",
      "    plt.imshow(values_plot, interpolation='none', vmax= 1, vmin = 0)\n",
      "    \n",
      "    xlabels = np.arange(0, len(states))\n",
      "    ylabels = np.arange(0, len(states[0]))\n",
      "\n",
      "    # Set the major ticks at the centers and minor tick at the edges\n",
      "    xlocs = np.arange(len(xlabels))\n",
      "    ylocs = np.arange(len(ylabels))\n",
      "    ax.xaxis.set_ticks(xlocs + 0.5, minor=True)\n",
      "    ax.xaxis.set(ticks=xlocs, ticklabels=xlabels)\n",
      "    ax.yaxis.set_ticks(ylocs + 0.5, minor=True)\n",
      "    ax.yaxis.set(ticks=ylocs, ticklabels=ylabels)\n",
      "    \n",
      "    # Turn on the grid for the minor ticks\n",
      "    ax.grid(True, which='minor', linestyle='-', linewidth=2)   \n",
      "    \n",
      "    for txt in ax.texts:\n",
      "        txt.set_visible(False)\n",
      "        \n",
      "    ax.annotate(\".\", ((position['x'] + 0.5)/len(states), (1-(position['y'] + 0.5)/len(states[0]))), size=160, textcoords='axes fraction', color='white')\n",
      "    plt.draw()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Main loop\n",
      "actions_executed = 0\n",
      "last_action_time = 0\n",
      "in_end_position = False\n",
      "\n",
      "# interactive plotting\n",
      "fig, ax = plt.subplots()\n",
      "plt.ion()\n",
      "    \n",
      "while actions_executed < NUM_ITERATIONS:\n",
      "    position = env.get_agent_pos().copy()\n",
      "\n",
      "    # plotting\n",
      "    plot_values(fig, ax, position)\n",
      "    \n",
      "    if not in_end_position:\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 1.})\n",
      "        \n",
      "        nest.SetStatus(wta_noise, {'rate': 3000.})\n",
      "        nest.Simulate(400)\n",
      "        max_rate = -1\n",
      "        chosen_action = -1\n",
      "        for i in range(len(sd_actions)):\n",
      "            rate = len([e for e in nest.GetStatus([sd_actions[i]], keys='events')[0]['times'] if e > last_action_time]) # calc the \"firerate\" of each actor population\n",
      "            if rate > max_rate:\n",
      "                max_rate = rate # the population with the hightes rate wins\n",
      "                chosen_action = i\n",
      "        nest.SetStatus(stimulus, {'rate': 5000.})\n",
      "\n",
      "        possible_actions = env.get_possible_actions() \n",
      "\n",
      "        new_position, outcome, in_end_position = env.move(possible_actions[chosen_action])\n",
      "\n",
      "        print \"iteration:\", actions_executed, \"action:\", chosen_action \n",
      "        print \"new pos:\", new_position, \"reward:\", outcome\n",
      "\n",
      "        print \"values\", np.mean(nest.GetStatus(nest.GetConnections(states[1][0], actions[0]), 'weight'))\n",
      "        print \"values\", np.mean(nest.GetStatus(nest.GetConnections(states[1][0], actions[1]), 'weight'))\n",
      "        \n",
      "        # stimulate new state\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 0.})\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[new_position['x']][new_position['y']]), {'weight': 1.})\n",
      "\n",
      "         \n",
      "        # apply reward\n",
      "        nest.SetStatus(nest.GetConnections(reward_stimulus, DA_neurons), {'weight': float(outcome)})\n",
      "        nest.SetStatus(wta_noise, {'rate': 0.})\n",
      "        \n",
      "        # refactory time\n",
      "        nest.Simulate(100.)\n",
      "        \n",
      "        nest.SetStatus(nest.GetConnections(reward_stimulus, DA_neurons), {'weight': 0.0})\n",
      "              \n",
      "        last_action_time += 500\n",
      "        actions_executed += 1\n",
      "    else:      \n",
      "        _, in_end_position = env.init_new_trial()\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 0.})\n",
      "\n",
      "\n",
      "rplt.from_device(sd_wta, title=\"WTA circuit\")\n",
      "rplt.from_device(sd_states, title=\"States\")\n",
      "rplt.from_device(sd_DA, title=\"DA pool\")\n",
      "rplt.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iteration: 0 action: 1\n",
        "new pos: {'y': 0, 'x': 2} reward: -1.0\n",
        "values -0.00138745103279\n",
        "values -0.0662148147505\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 action: 1\n",
        "new pos: {'y': 0, 'x': 2} reward: -1.0\n",
        "values -0.00180144289026\n",
        "values -0.145610895576\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 0.0294195485072\n",
        "values -0.159154677309\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 0.160321775676\n",
        "values -0.159253973403\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 action: 1\n",
        "new pos: {'y': 0, 'x': 2} reward: -1.0\n",
        "values 0.430822033647\n",
        "values -0.199268261434\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 0.39359469749\n",
        "values -0.22666954761\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 0.664988194374\n",
        "values -0.221209882576\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 0.844396012806\n",
        "values -0.221350157281\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 0.917374952526\n",
        "values -0.220523720257\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 1.16085239127\n",
        "values -0.21945115451\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 1.52829805197\n",
        "values -0.219091844874\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 1.85141195035\n",
        "values -0.217591015546\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 2.15285117085\n",
        "values -0.217453271856\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 2.34234128753\n",
        "values -0.216959926889\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 2.76184175578\n",
        "values -0.214732958949\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 3.1037780184\n",
        "values -0.214731383252\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 16 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 3.42893089605\n",
        "values -0.213884064853\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 17 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 3.72183707173\n",
        "values -0.213381427151\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 4.06585089645\n",
        "values -0.213381425892\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 19 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0\n",
        "values 4.30453549678\n",
        "values -0.213109149789\n",
        "iteration:"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}