{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dopaminergic Error Signal"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "In this notebook, we will introduce a more plausible way to signal the prediction error to the synapses by using a dopaminergic neuron pool."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "We want to move the neural circuit model towards more neural plausible implementation by replacing the artificial prediction error signal with a population of neurons that will model a dopaminergic (DA) neuron pool that release dopamine \u2013 a neurotransmitter known to be related to the prediction error signaling. Create a pool of N neurons and a dedicated volume transmitter \u2013 it is an NEST node that collects spikes from these neurons and translates them into a continuous low pass filter signal which models the dopaminergic release.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nest\n",
      "import nest.raster_plot as rplt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import environment as env\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "# you can switch environment by providing a specific index\n",
      "# 0 : 3 state binary choice task\n",
      "# 1 : 5 state task\n",
      "# 2 : 7 state task\n",
      "# 3 : experimental\n",
      "# 4 : experimental\n",
      "# 5 : experimental\n",
      "# 6 : 2x2 2D grid world\n",
      "# 7 : 3x3 2D grid world\n",
      "# 8 : 3x3 2D grid world with punishment\n",
      "# 9 : 4x4 2D grid world with punishment\n",
      "# 10 : 5x5 2D grid world with punishment\n",
      "env.set_environment(1)\n",
      "\n",
      "NUM_ITERATIONS = 2000\n",
      "LEARNING_RATE = 0.05\n",
      "NUM_STATE_NEURONS = 20\n",
      "NUM_WTA_NEURONS = 50\n",
      "WEIGHT_SCALING = 100 / NUM_STATE_NEURONS\n",
      "\n",
      "nest.ResetKernel()\n",
      "nest.set_verbosity(\"M_DEBUG\")\n",
      "\n",
      "rank = nest.Rank()\n",
      "size = nest.NumProcesses() \n",
      "seed = np.random.randint(0, 1000000)\n",
      "num_threads = 1\n",
      "nest.SetKernelStatus({\"local_num_threads\": num_threads})\n",
      "nest.SetKernelStatus({\"rng_seeds\": range(seed+num_threads * size + 1, seed + 2 * (num_threads * size) + 1),\n",
      "        \t\t      \"grng_seed\": seed+size+num_threads,\n",
      "                      \"resolution\": 0.1})\n",
      "\n",
      "# Create states\n",
      "world_dim = env.get_world_dimensions()\n",
      "states = []\n",
      "for i in range(world_dim['x']):\n",
      "    states.append([])\n",
      "    for j in range(world_dim['y']):\n",
      "        states[i].append(nest.Create('iaf_psc_alpha', NUM_STATE_NEURONS))\n",
      "all_states = np.ravel(states).tolist()\n",
      "\n",
      "# Create actions\n",
      "num_actions = env.get_num_possible_actions()\n",
      "actions = []\n",
      "for i in range(num_actions):\n",
      "    actions.append(nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS))\n",
      "all_actions = np.ravel(actions).tolist()\n",
      "\n",
      "# Create WTA circuit\n",
      "wta_ex_weights = 10.5\n",
      "wta_inh_weights = -2.6\n",
      "wta_ex_inh_weights = 2.8\n",
      "wta_noise_weights = 2.1\n",
      "\n",
      "wta_inh_neurons = nest.Create('iaf_psc_alpha', NUM_WTA_NEURONS)\n",
      "\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], actions[i], 'all_to_all', {'weight': wta_ex_weights})\n",
      "    nest.Connect(actions[i], wta_inh_neurons, 'all_to_all', {'weight': wta_ex_inh_weights}) \n",
      "\n",
      "nest.Connect(wta_inh_neurons, all_actions, 'all_to_all', {'weight': wta_inh_weights})\n",
      "\n",
      "wta_noise = nest.Create('poisson_generator', 10, {'rate': 3000.})\n",
      "nest.Connect(wta_noise, all_actions, 'all_to_all', {'weight': wta_noise_weights})\n",
      "nest.Connect(wta_noise, wta_inh_neurons, 'all_to_all', {'weight': wta_noise_weights * 0.9})\n",
      "\n",
      "\n",
      "# Create stimulus\n",
      "stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "nest.Connect(stimulus, all_states, 'all_to_all', {'weight': 0.})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gamma = 0.8\n",
      "def update_values(position, chosen_action, new_position, outcome):\n",
      "    # prediction error\n",
      "    current_value = np.mean(nest.GetStatus(nest.GetConnections(states[position['x']][position['y']], actions[chosen_action]), 'weight'))\n",
      "    new_value = np.max([np.mean(nest.GetStatus(nest.GetConnections(states[new_position['x']][new_position['y']], actions[a]), 'weight')) for a in range(len(actions))])\n",
      "    prediction_error = outcome + gamma * new_value - current_value\n",
      "    \n",
      "    # update values\n",
      "    return prediction_error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Here, we are implementing the dopaminergic nueron pool, volume transmitter and dopamin-modulated synapse between states and actions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create DA pool\n",
      "DA_neurons = nest.Create('iaf_psc_alpha', 100)\n",
      "vol_trans = nest.Create('volume_transmitter', 1, {'deliver_interval': 50})\n",
      "nest.Connect(DA_neurons, vol_trans, 'all_to_all')\n",
      "\n",
      "# Create reward stimulus\n",
      "reward_stimulus = nest.Create('poisson_generator', 1, {'rate': 5000.})\n",
      "nest.Connect(reward_stimulus, DA_neurons, 'all_to_all', {'weight': 0.})\n",
      "\n",
      "tau_c = 50.0\n",
      "tau_n = 20.0\n",
      "\n",
      "# Connect states to actions\n",
      "nest.CopyModel('neuromod_dopa_synapse', 'dopa_synapse', {'vt': vol_trans[0], 'A_LTP': 20., 'A_LTD': 25.,\n",
      "                                                         'Wmin': -30., 'Wmax': 30., 'b': 1.0, 'tau_n': tau_n, 'tau_c': tau_c}) #0.95\n",
      "        \n",
      "nest.Connect(all_states, all_actions, 'all_to_all', {'model': 'dopa_synapse', 'weight': 0.0})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create spike detector\n",
      "sd_wta = nest.Create('spike_detector')\n",
      "nest.Connect(all_actions, sd_wta)\n",
      "nest.Connect(wta_inh_neurons, sd_wta)\n",
      "sd_actions = nest.Create('spike_detector', num_actions)\n",
      "for i in range(len(actions)):\n",
      "    nest.Connect(actions[i], [sd_actions[i]])\n",
      "sd_states = nest.Create('spike_detector')\n",
      "nest.Connect(all_states, sd_states)\n",
      "sd_DA = nest.Create('spike_detector', 1)\n",
      "nest.Connect(DA_neurons, sd_DA, 'all_to_all')\n",
      "\n",
      "\n",
      "# Create noise\n",
      "noise = nest.Create('poisson_generator', 1, {'rate': 65000.})\n",
      "nest.Connect(noise, all_states, 'all_to_all', {'weight': 1.})\n",
      "nest.Connect(noise, DA_neurons, 'all_to_all', {'weight': 1.0375})\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_values(fig, ax, position):\n",
      "    plt.cla()\n",
      "    \n",
      "    values_plot = []      \n",
      "    \n",
      "    for i in range(world_dim['y']):\n",
      "        values_plot.append([])\n",
      "        for j in range(world_dim['x']):\n",
      "            values_plot[i].append(np.mean([np.mean(nest.GetStatus(nest.GetConnections(states[j][i], actions[a]), 'weight')) for a in range(len(actions))]))\n",
      "            if len(actions) == 4:\n",
      "                q_north = np.mean(nest.GetStatus(nest.GetConnections(states[j][i], actions[0]), 'weight'))\n",
      "                q_east = np.mean(nest.GetStatus(nest.GetConnections(states[j][i], actions[1]), 'weight'))\n",
      "                q_south = np.mean(nest.GetStatus(nest.GetConnections(states[j][i], actions[2]), 'weight'))\n",
      "                q_west = np.mean(nest.GetStatus(nest.GetConnections(states[j][i], actions[3]), 'weight'))\n",
      "                ax.arrow(j, i, (q_east-q_west)/10., (q_south-q_north)/10., head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
      "            else:\n",
      "                q_east = np.mean(nest.GetStatus(nest.GetConnections(states[j][i], actions[0]), 'weight'))\n",
      "                q_west = np.mean(nest.GetStatus(nest.GetConnections(states[j][i], actions[1]), 'weight'))\n",
      "                ax.arrow(j, i, (q_west-q_east)/10., 0., head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
      "\n",
      "    \n",
      "    values_plot = np.array(values_plot)\n",
      "    \n",
      "    plt.imshow(values_plot, interpolation='none', vmax=1 * WEIGHT_SCALING, vmin=-1 * WEIGHT_SCALING)\n",
      "    \n",
      "    xlabels = np.arange(0, len(states))\n",
      "    ylabels = np.arange(0, len(states[0]))\n",
      "\n",
      "    # Set the major ticks at the centers and minor tick at the edges\n",
      "    xlocs = np.arange(len(xlabels))\n",
      "    ylocs = np.arange(len(ylabels))\n",
      "    ax.xaxis.set_ticks(xlocs + 0.5, minor=True)\n",
      "    ax.xaxis.set(ticks=xlocs, ticklabels=xlabels)\n",
      "    ax.yaxis.set_ticks(ylocs + 0.5, minor=True)\n",
      "    ax.yaxis.set(ticks=ylocs, ticklabels=ylabels)\n",
      "    \n",
      "    # Turn on the grid for the minor ticks\n",
      "    ax.grid(True, which='minor', linestyle='-', linewidth=2)   \n",
      "    \n",
      "    for txt in ax.texts:\n",
      "        txt.set_visible(False)\n",
      "        \n",
      "    ax.annotate(\".\", ((position['x'] + 0.5)/len(states), (1-(position['y'] + 0.5)/len(states[0]))), size=160, textcoords='axes fraction', color='white')\n",
      "    plt.draw()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# Main loop\n",
      "actions_executed = 0\n",
      "last_action_time = 0\n",
      "in_end_position = False\n",
      "\n",
      "# interactive plotting\n",
      "fig, ax = plt.subplots()\n",
      "plt.ion()\n",
      "    \n",
      "while actions_executed < NUM_ITERATIONS:\n",
      "    position = env.get_agent_pos().copy()\n",
      "\n",
      "    # plotting\n",
      "    plot_values(fig, ax, position)\n",
      "    \n",
      "    if not in_end_position:\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 1.})\n",
      "        \n",
      "        nest.SetStatus(wta_noise, {'rate': 3000.})\n",
      "        nest.Simulate(200)\n",
      "        max_rate = -1\n",
      "        chosen_action = -1\n",
      "        for i in range(len(sd_actions)):\n",
      "            rate = len([e for e in nest.GetStatus([sd_actions[i]], keys='events')[0]['times'] if e > last_action_time]) # calc the \"firerate\" of each actor population\n",
      "            if rate > max_rate:\n",
      "                max_rate = rate # the population with the hightes rate wins\n",
      "                chosen_action = i\n",
      "        nest.SetStatus(stimulus, {'rate': 5000.})\n",
      "\n",
      "        possible_actions = env.get_possible_actions() \n",
      "\n",
      "        new_position, outcome, in_end_position = env.move(possible_actions[chosen_action])\n",
      "\n",
      "        prediction_error = update_values(position, chosen_action, new_position, outcome * WEIGHT_SCALING)\n",
      "\n",
      "        print \"iteration:\", actions_executed, \"action:\", chosen_action \n",
      "        print \"new pos:\", new_position, \"reward:\", outcome, \"prediction error:\", prediction_error\n",
      "\n",
      "        policy = []\n",
      "        policy.append(np.mean(nest.GetStatus(nest.GetConnections(states[1][0], actions[0]), 'weight')))\n",
      "        policy.append(np.mean(nest.GetStatus(nest.GetConnections(states[1][0], actions[1]), 'weight')))\n",
      "             \n",
      "        # stimulate new state\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 0.})\n",
      "\n",
      "         \n",
      "        # apply reward\n",
      "        nest.SetStatus(nest.GetConnections(reward_stimulus, DA_neurons), {'weight': float(prediction_error)})\n",
      "        nest.SetStatus(wta_noise, {'rate': 0.})\n",
      "        \n",
      "        # refactory time\n",
      "        nest.Simulate(50.)\n",
      "        \n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[new_position['x']][new_position['y']]), {'weight': 1.})\n",
      "        nest.SetStatus(nest.GetConnections(reward_stimulus, DA_neurons), {'weight': 0.0})\n",
      "              \n",
      "        last_action_time += 250\n",
      "        actions_executed += 1\n",
      "    else:      \n",
      "        _, in_end_position = env.init_new_trial()\n",
      "        nest.SetStatus(nest.GetConnections(stimulus, states[position['x']][position['y']]), {'weight': 0.})\n",
      "\n",
      "\n",
      "rplt.from_device(sd_wta, title=\"WTA circuit\")\n",
      "rplt.from_device(sd_states, title=\"States\")\n",
      "rplt.from_device(sd_DA, title=\"DA pool\")\n",
      "rplt.show()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/usr/lib/pymodules/python2.7/matplotlib/patches.py:1054: RuntimeWarning: invalid value encountered in double_scalars\n",
        "/usr/lib/pymodules/python2.7/matplotlib/patches.py:1055: RuntimeWarning: invalid value encountered in double_scalars\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 action: 1\n",
        "new pos: {'y': 0, 'x': 3} reward: 0 prediction error: -0.000162815479568\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 action: 0\n",
        "new pos: {'y': 0, 'x': 2} reward: 0 prediction error: -0.0141753928779\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 action: 0\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 prediction error: -0.00735322553319\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0 prediction error: 5.00046547144\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4 action: 0\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 prediction error: 0.0414448521721\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0 prediction error: 4.90779780773\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6 action: 0\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 prediction error: 0.163869992814\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7 action: 0\n",
        "new pos: {'y': 0, 'x': 0} reward: 1.0 prediction error: 4.64776677716\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8 action: 1\n",
        "new pos: {'y': 0, 'x': 3} reward: 0 prediction error: 0.0373800949107\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9 action: 0\n",
        "new pos: {'y': 0, 'x': 2} reward: 0 prediction error: 0.0771887805887\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10 action: 0\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 prediction error: 0.403263827836\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 11 action: 1\n",
        "new pos: {'y': 0, 'x': 2} reward: 0 prediction error: 0.123605050629\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12 action: 1\n",
        "new pos: {'y': 0, 'x': 3} reward: 0 prediction error: -0.0449255001918\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13 action: 0\n",
        "new pos: {'y': 0, 'x': 2} reward: 0 prediction error: 0.079140286724\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 14 action: 0\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 prediction error: 0.442155451143\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15 action: 1\n",
        "new pos: {'y': 0, 'x': 2} reward: 0 prediction error: 0.130570855934\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 16 action: 1\n",
        "new pos: {'y': 0, 'x': 3} reward: 0 prediction error: -0.07372762433\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 17 action: 0\n",
        "new pos: {'y': 0, 'x': 2} reward: 0 prediction error: 0.0787457660658\n",
        "iteration:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 18 action: 0\n",
        "new pos: {'y': 0, 'x': 1} reward: 0 prediction error: 0.430535080675\n",
        "iteration:"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise: Provide plots that give insight about the learning progress. Observe how dopaminergic (DA) neuron pool represents the prediction error with its activity. Visualize how DA pool response to reward evolves during the learning. Experiment around with providing different reward magnitudes or having a punishing state instead of a neutral one. Describe you observations. Are there any issues with learning from punishment? If yes, is there any workaround you can think of? "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}